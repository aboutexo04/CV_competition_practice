# src/train.py

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import copy
from tqdm import tqdm
from sklearn.metrics import f1_score, classification_report
from sklearn.model_selection import StratifiedKFold
from torch.utils.data import DataLoader, Subset, WeightedRandomSampler
from collections import Counter
from src.model import LabelSmoothingLoss


class EarlyStopping:
    def __init__(self, patience=5, verbose=True, delta=0):
        """
        Args:
            patience (int): validation F1ì´ ê°œì„ ë˜ì§€ ì•Šì•„ë„ ê¸°ë‹¤ë¦´ epoch ìˆ˜
            verbose (bool): ë©”ì‹œì§€ ì¶œë ¥ ì—¬ë¶€
            delta (float): ê°œì„ ìœ¼ë¡œ ì¸ì •í•  ìµœì†Œ ë³€í™”ëŸ‰
        """
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_f1 = 0
        self.best_model_state = None
        
    def __call__(self, val_f1, model):
        score = val_f1
        
        if self.best_score is None:
            self.best_score = score
            self.best_f1 = val_f1
            self.save_checkpoint(val_f1, model)
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.verbose:
                print(f'â¸ï¸  EarlyStopping counter: {self.counter}/{self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.best_f1 = val_f1
            self.save_checkpoint(val_f1, model)
            self.counter = 0
    
    def save_checkpoint(self, val_f1, model):
        '''validation F1ì´ ê°œì„ ë˜ë©´ ëª¨ë¸ ì €ì¥'''
        if self.verbose:
            print(f'âœ… Validation F1 improved ({self.best_f1:.4f} â†’ {val_f1:.4f})')
        self.best_model_state = copy.deepcopy(model.state_dict())
        self.best_f1 = val_f1


class TransformSubset(torch.utils.data.Dataset):
    """Transformì„ ì ìš©í•˜ëŠ” Subset wrapper"""
    def __init__(self, subset, transform):
        self.subset = subset
        self.transform = transform
    
    def __len__(self):
        return len(self.subset)
    
    def __getitem__(self, idx):
        img, label = self.subset[idx]
        if self.transform:
            augmented = self.transform(image=img)
            img = augmented['image']
        return img, label


def safe_collate_fn(batch):
    """ì±„ë„ ìˆ˜ ì•ˆì „ ì¥ì¹˜"""
    import torch
    
    images = []
    labels = []
    
    for image, label in batch:
        # ì±„ë„ í™•ì¸
        if image.ndim == 2:  # (H, W)
            image = image.unsqueeze(0).repeat(3, 1, 1)
        elif image.shape[0] == 1:  # (1, H, W)
            image = image.repeat(3, 1, 1)
        elif image.shape[0] == 4:  # (4, H, W) - RGBA
            image = image[:3, :, :]
        
        images.append(image)
        labels.append(label)
    
    images = torch.stack(images, dim=0)
    labels = torch.tensor(labels, dtype=torch.long)
    
    return images, labels


def train_one_epoch(model, train_loader, criterion, optimizer, device):
    """
    í•œ ì—í­ ë™ì•ˆ ëª¨ë¸ í•™ìŠµ
    
    Args:
        model: í•™ìŠµí•  ëª¨ë¸
        train_loader: í•™ìŠµ ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        optimizer: ì˜µí‹°ë§ˆì´ì €
        device: ë””ë°”ì´ìŠ¤
    
    Returns:
        train_loss, train_acc, train_f1
    """
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    
    for images, labels in tqdm(train_loader, desc="Training", leave=False):
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item() * images.size(0)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
    
    train_loss = running_loss / total
    train_acc = 100. * correct / total
    train_f1 = f1_score(all_labels, all_preds, average='macro')
    
    return train_loss, train_acc, train_f1


def validate(model, val_loader, criterion, device):
    """
    ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ í‰ê°€
    
    Args:
        model: í‰ê°€í•  ëª¨ë¸
        val_loader: ê²€ì¦ ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        device: ë””ë°”ì´ìŠ¤
    
    Returns:
        val_loss, val_acc, val_f1, all_preds, all_labels
    """
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in tqdm(val_loader, desc="Validation", leave=False):
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item() * images.size(0)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    val_loss = running_loss / total
    val_acc = 100. * correct / total
    val_f1 = f1_score(all_labels, all_preds, average='macro')

    return val_loss, val_acc, val_f1, all_preds, all_labels


def analyze_opinion_class(val_labels, val_preds, opinion_class_id, num_classes):
    """
    Statement of Opinion í´ë˜ìŠ¤ ìƒì„¸ ë¶„ì„
    
    Args:
        val_labels: ì‹¤ì œ ë ˆì´ë¸”
        val_preds: ì˜ˆì¸¡ ë ˆì´ë¸”
        opinion_class_id: Statement of Opinion í´ë˜ìŠ¤ ID
        num_classes: ì „ì²´ í´ë˜ìŠ¤ ìˆ˜
    """
    from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
    from collections import Counter
    
    print("\n" + "="*70)
    print("ğŸ’­ Statement of Opinion ìƒì„¸ ë¶„ì„")
    print("="*70)
    
    # ê¸°ë³¸ í†µê³„
    val_labels_np = np.array(val_labels)
    val_preds_np = np.array(val_preds)
    
    opinion_actual_mask = val_labels_np == opinion_class_id
    opinion_preds_mask = val_preds_np == opinion_class_id
    
    print(f"ì‹¤ì œ Opinion ìƒ˜í”Œ: {opinion_actual_mask.sum()}ê°œ")
    print(f"ì˜ˆì¸¡ëœ Opinion: {opinion_preds_mask.sum()}ê°œ")
    
    # Precision, Recall, F1
    p, r, f1, support = precision_recall_fscore_support(
        val_labels, val_preds, 
        labels=[opinion_class_id],
        zero_division=0
    )
    
    print(f"\nğŸ“Š ì„±ëŠ¥ ì§€í‘œ:")
    print(f"   Precision: {p[0]:.3f} (ì˜ˆì¸¡í•œ Opinion ì¤‘ ë§ì¶˜ ë¹„ìœ¨)")
    print(f"   Recall:    {r[0]:.3f} (ì‹¤ì œ Opinion ì¤‘ ì°¾ì•„ë‚¸ ë¹„ìœ¨)")
    print(f"   F1 Score:  {f1[0]:.3f}")
    
    # Opinionì„ ë‹¤ë¥¸ í´ë˜ìŠ¤ë¡œ ì˜¤ë¶„ë¥˜í•œ ê²½ìš° (Recall ë¬¸ì œ)
    if opinion_actual_mask.sum() > 0:
        opinion_samples_preds = val_preds_np[opinion_actual_mask]
        misclassified = opinion_samples_preds != opinion_class_id
        
        print(f"\nâŒ Opinionì„ ë†“ì¹œ ê²½ìš°: {misclassified.sum()}/{opinion_actual_mask.sum()}ê°œ")
        
        if misclassified.sum() > 0:
            wrong_preds = opinion_samples_preds[misclassified]
            wrong_counts = Counter(wrong_preds)
            print(f"   ì–´ë–¤ í´ë˜ìŠ¤ë¡œ ì˜¤ë¶„ë¥˜ë˜ì—ˆë‚˜:")
            for class_id, count in wrong_counts.most_common(5):
                percentage = 100 * count / misclassified.sum()
                print(f"   â†’ Class {class_id}: {count}ë²ˆ ({percentage:.1f}%)")
    
    # ë‹¤ë¥¸ í´ë˜ìŠ¤ë¥¼ Opinionìœ¼ë¡œ ì˜¤ë¶„ë¥˜í•œ ê²½ìš° (Precision ë¬¸ì œ)
    false_positives = (val_preds_np == opinion_class_id) & (val_labels_np != opinion_class_id)
    
    if false_positives.sum() > 0:
        print(f"\nâš ï¸  ë‹¤ë¥¸ í´ë˜ìŠ¤ë¥¼ Opinionìœ¼ë¡œ ì˜¤íŒ: {false_positives.sum()}ê°œ")
        false_positive_labels = val_labels_np[false_positives]
        fp_counts = Counter(false_positive_labels)
        print(f"   ì–´ë–¤ í´ë˜ìŠ¤ê°€ Opinionìœ¼ë¡œ ì˜¤íŒë˜ì—ˆë‚˜:")
        for class_id, count in fp_counts.most_common(5):
            percentage = 100 * count / false_positives.sum()
            print(f"   â†’ Class {class_id}: {count}ë²ˆ ({percentage:.1f}%)")
    
    # ì§„ë‹¨ ë° ê¶Œì¥ì‚¬í•­
    print(f"\nğŸ’¡ ì§„ë‹¨ ë° ê¶Œì¥ì‚¬í•­:")
    
    if r[0] < 0.5:
        print(f"   ğŸ”´ Recall({r[0]:.3f})ì´ ë§¤ìš° ë‚®ìŒ â†’ Opinionì„ ëª» ì°¾ê³  ìˆìŒ")
        print(f"      â†’ Class Weight ê°•í™” ë˜ëŠ” Oversampling í•„ìš”")
        print(f"      â†’ Config: MANUAL_CLASS_WEIGHTS = {{{opinion_class_id}: 3.0}}")
    elif r[0] < 0.7:
        print(f"   ğŸŸ¡ Recall({r[0]:.3f})ì´ ë‚®ì€ í¸ â†’ Opinion ê²€ì¶œ ê°œì„  í•„ìš”")
        print(f"      â†’ Class Weight ì ìš© ê¶Œì¥")
    
    if p[0] < 0.5:
        print(f"   ğŸ”´ Precision({p[0]:.3f})ì´ ë§¤ìš° ë‚®ìŒ â†’ ë„ˆë¬´ ë§ì´ Opinionìœ¼ë¡œ ì˜ˆì¸¡")
        print(f"      â†’ ëª¨ë¸ capacity ì¦ê°€ ë˜ëŠ” í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ì™€ì˜ êµ¬ë¶„ í•™ìŠµ")
    elif p[0] < 0.7:
        print(f"   ğŸŸ¡ Precision({p[0]:.3f})ì´ ë‚®ì€ í¸ â†’ ì˜¤íŒ ì¤„ì´ê¸° í•„ìš”")
    
    if r[0] >= 0.7 and p[0] >= 0.7:
        print(f"   ğŸŸ¢ ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸ â†’ ë¯¸ì„¸ ì¡°ì •ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥")
    
    print("="*70)


def run_kfold_training(train_dataset_raw, train_labels, config):
    """
    K-Fold Cross Validation í•™ìŠµ ì‹¤í–‰ (Config ê¸°ë°˜)
    
    Args:
        train_dataset_raw: Transformì´ ì ìš©ë˜ì§€ ì•Šì€ ì›ë³¸ dataset
        train_labels: Train labels
        config: Config ê°ì²´
        
    Returns:
        fold_results: ê° foldë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    from src.data import get_train_augmentation, get_val_augmentation
    from src.model import get_model, get_optimizer
    
    # Configì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    device = config.DEVICE
    num_classes = config.NUM_CLASSES
    use_wandb = config.USE_WANDB
    n_folds = config.N_FOLDS
    epochs = config.EPOCHS
    batch_size = config.BATCH_SIZE
    model_name = config.MODEL_NAME
    patience = config.PATIENCE
    
    # Statement of Opinion í´ë˜ìŠ¤ ID (configì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’)
    opinion_class_id = getattr(config, 'OPINION_CLASS_ID', None)
    
    print("=" * 70)
    print("ğŸš€ K-Fold Cross Validation ì‹œì‘")
    print(f"Model: {model_name}, Epochs: {epochs}, Batch: {batch_size}, Folds: {n_folds}")
    if opinion_class_id is not None:
        print(f"Statement of Opinion í´ë˜ìŠ¤ ID: {opinion_class_id}")
    print("=" * 70)
    
    # âœ… Augmentation ìˆ˜ì •!
    train_transform = get_train_augmentation(config.IMAGE_SIZE, config)
    val_transform = get_val_augmentation(config.IMAGE_SIZE)
    
    # K-Fold ì„¤ì •
    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset_raw, train_labels)):
        print(f"\n{'='*70}")
        print(f"ğŸ“ Fold {fold + 1}/{n_folds}")
        print(f"{'='*70}")
        
        # Dataset & DataLoader
        train_subset = Subset(train_dataset_raw, train_idx)
        val_subset = Subset(train_dataset_raw, val_idx)
        
        # Transform ì ìš©
        train_dataset = TransformSubset(train_subset, train_transform)
        val_dataset = TransformSubset(val_subset, val_transform)

        # Class-Balanced Sampling ì„¤ì •
        use_balanced_sampling = getattr(config, 'USE_CLASS_BALANCED_SAMPLING', False)
        sampler = None
        shuffle = True

        if use_balanced_sampling:
            # í˜„ì¬ foldì˜ train ë ˆì´ë¸” ì¶”ì¶œ
            train_labels_fold = [train_labels[i] for i in train_idx]

            # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
            class_counts = Counter(train_labels_fold)

            # ê° í´ë˜ìŠ¤ì˜ ê°€ì¤‘ì¹˜ ê³„ì‚° (inverse frequency)
            class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}

            # ê° ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
            sample_weights = [class_weights[label] for label in train_labels_fold]

            # WeightedRandomSampler ìƒì„±
            sampler = WeightedRandomSampler(
                weights=sample_weights,
                num_samples=len(sample_weights),
                replacement=True  # ë³µì› ì¶”ì¶œ (oversampling íš¨ê³¼)
            )
            shuffle = False  # sampler ì‚¬ìš© ì‹œ shuffleì€ False

            if fold == 0:  # ì²« ë²ˆì§¸ foldì—ì„œë§Œ ì¶œë ¥
                print(f"\nâœ… Class-Balanced Sampling í™œì„±í™”")
                print(f"   ìƒ˜í”Œë³„ ê°€ì¤‘ì¹˜ë¡œ ê· í˜•ì¡íŒ ìƒ˜í”Œë§ ìˆ˜í–‰")
                # ê°€ì¥ ì ì€ í´ë˜ìŠ¤ì™€ ê°€ì¥ ë§ì€ í´ë˜ìŠ¤ ë¹„êµ
                sorted_classes = sorted(class_counts.items(), key=lambda x: x[1])
                min_class_id, min_count = sorted_classes[0]
                max_class_id, max_count = sorted_classes[-1]

                min_weight = class_weights[min_class_id]
                max_weight = class_weights[max_class_id]
                sampling_ratio = min_weight / max_weight  # ìµœì†Œ í´ë˜ìŠ¤ê°€ ìµœëŒ€ í´ë˜ìŠ¤ ëŒ€ë¹„ ëª‡ ë°° ë” ìì£¼ ìƒ˜í”Œë§ë˜ëŠ”ì§€

                print(f"   í´ë˜ìŠ¤ë³„ ìƒ˜í”Œë§ ê°€ì¤‘ì¹˜:")
                print(f"     ìµœì†Œ í´ë˜ìŠ¤ {min_class_id}: {min_count}ê°œ â†’ weight {min_weight:.4f}")
                print(f"     ìµœëŒ€ í´ë˜ìŠ¤ {max_class_id}: {max_count}ê°œ â†’ weight {max_weight:.4f}")
                print(f"     â†’ ìµœì†Œ í´ë˜ìŠ¤ê°€ ìµœëŒ€ í´ë˜ìŠ¤ë³´ë‹¤ {sampling_ratio:.1f}ë°° ë” ìì£¼ ìƒ˜í”Œë§ë¨")

        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            sampler=sampler,
            num_workers=0,
            collate_fn=safe_collate_fn
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,
            collate_fn=safe_collate_fn
        )
        
        print(f"Train: {len(train_dataset):,}, Val: {len(val_dataset):,}")

        # ëª¨ë¸ ìƒì„± (ë§¤ foldë§ˆë‹¤ ìƒˆë¡œ ìƒì„±)
        dropout_rate = getattr(config, 'DROPOUT_RATE', 0.0)
        model = get_model(
            model_name,
            num_classes,
            pretrained=True,
            dropout_rate=dropout_rate
        ).to(device)
        optimizer = get_optimizer(model, config)
        
        # Class weights ê³„ì‚° (í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘)
        use_class_weights = getattr(config, 'USE_CLASS_WEIGHTS', False)
        class_weights = None

        if use_class_weights:
            # í˜„ì¬ foldì˜ train ë ˆì´ë¸”ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°
            train_labels_fold = [train_labels[i] for i in train_idx]
            class_counts = Counter(train_labels_fold)

            # Power íŒŒë¼ë¯¸í„°ë¡œ ê°€ì¤‘ì¹˜ ì¡°ì ˆ
            power = getattr(config, 'CLASS_WEIGHT_POWER', 0.5)
            
            # Inverse frequency weighting with power
            total_samples = len(train_labels_fold)
            weights = []
            for class_id in range(num_classes):
                count = class_counts.get(class_id, 1)  # 0 ë°©ì§€
                weight = total_samples / (num_classes * count)
                # Powerë¡œ ì™„í™” (1.0ì´ë©´ ì›ë˜ëŒ€ë¡œ, 0.5ë©´ ì œê³±ê·¼)
                weight = weight ** power
                weights.append(weight)
            
            # ìˆ˜ë™ weight ì˜¤ë²„ë¼ì´ë“œ
            manual_weights = getattr(config, 'MANUAL_CLASS_WEIGHTS', {})
            if manual_weights:
                for class_id, multiplier in manual_weights.items():
                    weights[class_id] *= multiplier

            class_weights = torch.FloatTensor(weights).to(device)

            if fold == 0:
                print(f"\nâœ… Class Weights ì ìš© (power={power}):")
                # ìƒ˜í”Œ ìˆ˜ê°€ ì ì€ ìƒìœ„ 5ê°œ í´ë˜ìŠ¤ í‘œì‹œ
                sorted_classes = sorted(class_counts.items(), key=lambda x: x[1])
                for class_id, count in sorted_classes[:5]:
                    print(f"   Class {class_id}: {count}ê°œ â†’ weight {weights[class_id]:.3f}")
                print(f"   Min weight: {min(weights):.3f}, Max weight: {max(weights):.3f}")
                
                if manual_weights:
                    print(f"\nğŸ¯ Manual weight override:")
                    for class_id, mult in manual_weights.items():
                        print(f"   Class {class_id}: Ã—{mult} â†’ {weights[class_id]:.3f}")

        # Loss í•¨ìˆ˜ ì„ íƒ
        if config.USE_LABEL_SMOOTHING:
            criterion = LabelSmoothingLoss(
                num_classes=num_classes,
                smoothing=config.LABEL_SMOOTHING_FACTOR
            )
            if fold == 0:  # ì²« ë²ˆì§¸ foldì—ì„œë§Œ ì¶œë ¥
                print(f"\nâœ… Label Smoothing ì‚¬ìš© (smoothing={config.LABEL_SMOOTHING_FACTOR})")
        else:
            # Class weights ì ìš©
            if class_weights is not None:
                criterion = nn.CrossEntropyLoss(weight=class_weights)
            else:
                criterion = nn.CrossEntropyLoss()

            if fold == 0:
                weight_msg = " with Class Weights" if class_weights is not None else ""
                print(f"\nâœ… CrossEntropyLoss ì‚¬ìš©{weight_msg}")
        
        # Scheduler & Early Stopping
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=2, verbose=True
        )
        early_stopping_delta = getattr(config, 'EARLY_STOPPING_DELTA', 0.001)
        early_stopping = EarlyStopping(patience=patience, verbose=True, delta=early_stopping_delta)
        
        # í•™ìŠµ ì´ë ¥
        history = {
            'train_loss': [], 'train_acc': [], 'train_f1': [],
            'val_loss': [], 'val_acc': [], 'val_f1': []
        }
        
        # í•™ìŠµ ë£¨í”„
        for epoch in range(epochs):
            print(f"\nğŸ“ Epoch [{epoch+1}/{epochs}]")
            
            # Train
            train_loss, train_acc, train_f1 = train_one_epoch(
                model, train_loader, criterion, optimizer, device
            )
            
            # Validation
            val_loss, val_acc, val_f1, val_preds, val_labels = validate(
                model, val_loader, criterion, device
            )
            
            # Scheduler step
            scheduler.step(val_f1)
            
            # ì´ë ¥ ì €ì¥
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['train_f1'].append(train_f1)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            history['val_f1'].append(val_f1)
            
            # ì¶œë ¥
            print(f"Train - Loss: {train_loss:.4f} | Acc: {train_acc:.2f}% | F1: {train_f1:.4f}")
            print(f"Val   - Loss: {val_loss:.4f} | Acc: {val_acc:.2f}% | F1: {val_f1:.4f}")
            
            # Statement of Opinion ìƒì„¸ ë¶„ì„ (ì²« fold, ë§ˆì§€ë§‰ epoch ë˜ëŠ” early stop ì‹œ)
            if opinion_class_id is not None and fold == 0:
                if epoch == epochs - 1 or (epoch > 0 and early_stopping.early_stop):
                    analyze_opinion_class(val_labels, val_preds, opinion_class_id, num_classes)
            
            # Wandb ë¡œê¹…
            if use_wandb:
                import wandb
                wandb.log({
                    f"fold_{fold+1}/train_loss": train_loss,
                    f"fold_{fold+1}/train_acc": train_acc,
                    f"fold_{fold+1}/train_f1": train_f1,
                    f"fold_{fold+1}/val_loss": val_loss,
                    f"fold_{fold+1}/val_acc": val_acc,
                    f"fold_{fold+1}/val_f1": val_f1,
                    f"fold_{fold+1}/epoch": epoch + 1
                })
            
            # Early Stopping
            early_stopping(val_f1, model)
            if early_stopping.early_stop:
                print("ğŸ›‘ Early stopping!")
                break
        
        # Fold ê²°ê³¼ ì €ì¥
        fold_results.append({
            'fold': fold + 1,
            'best_val_f1': early_stopping.best_f1,
            'best_model_state': early_stopping.best_model_state,
            'history': history
        })
        
        print(f"\nâœ… Fold {fold + 1} ì™„ë£Œ! Best Val F1: {early_stopping.best_f1:.4f}")
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“Š K-Fold ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    
    for result in fold_results:
        print(f"Fold {result['fold']}: Best Val F1 = {result['best_val_f1']:.4f}")
    
    avg_f1 = np.mean([r['best_val_f1'] for r in fold_results])
    std_f1 = np.std([r['best_val_f1'] for r in fold_results])
    
    print("=" * 70)
    print(f"ğŸ“ˆ í‰ê·  Validation F1: {avg_f1:.4f} Â± {std_f1:.4f}")
    print("=" * 70)
    
    # Wandb ìµœì¢… ë¡œê¹…
    if use_wandb:
        import wandb
        wandb.log({
            "cv/avg_val_f1": avg_f1,
            "cv/std_val_f1": std_f1
        })

    # ============================================
    # Save Best Fold Model (if enabled)
    # ============================================
    save_model = getattr(config, 'SAVE_MODEL', False)

    if save_model:
        # Find best fold
        best_fold_idx = np.argmax([r['best_val_f1'] for r in fold_results])
        best_fold = fold_results[best_fold_idx]
        best_f1 = best_fold['best_val_f1']

        print("\n" + "=" * 70)
        print("ğŸ’¾ Saving Best Model")
        print("=" * 70)
        print(f"Best Fold: {best_fold['fold']} (Val F1: {best_f1:.4f})")

        # Create models directory
        from pathlib import Path
        models_dir = Path(getattr(config, 'MODELS_DIR', 'models'))
        models_dir.mkdir(exist_ok=True)

        # Save model with descriptive filename
        model_filename = f"{model_name}_best_f1_{best_f1:.4f}.pth"
        model_path = models_dir / model_filename

        torch.save(best_fold['best_model_state'], model_path)

        print(f"âœ… Model saved: {model_path}")
        print("=" * 70)
    else:
        print("\nâ­ï¸  Model saving disabled (SAVE_MODEL=False)")

    return fold_results