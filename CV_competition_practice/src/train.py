import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import copy
from tqdm import tqdm
from sklearn.metrics import f1_score

# Config import
try:
    from src.config import PATIENCE, EPOCHS, BATCH_SIZE, LR, N_FOLDS
except ImportError:
    # ê¸°ë³¸ê°’ ì„¤ì •
    PATIENCE = 5
    EPOCHS = 10
    BATCH_SIZE = 32
    LR = 0.001
    N_FOLDS = 5


class EarlyStopping:
    def __init__(self, patience=5, verbose=True, delta=0):
        """
        Args:
            patience (int): validation F1ì´ ê°œì„ ë˜ì§€ ì•Šì•„ë„ ê¸°ë‹¤ë¦´ epoch ìˆ˜
            verbose (bool): ë©”ì‹œì§€ ì¶œë ¥ ì—¬ë¶€
            delta (float): ê°œì„ ìœ¼ë¡œ ì¸ì •í•  ìµœì†Œ ë³€í™”ëŸ‰
        """
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_f1 = 0
        
    def __call__(self, val_f1, model):
        score = val_f1
        
        if self.best_score is None:
            self.best_score = score
            self.best_f1 = val_f1
            self.save_checkpoint(val_f1, model)
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.verbose:
                print(f'â¸ï¸  EarlyStopping counter: {self.counter}/{self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.best_f1 = val_f1
            self.save_checkpoint(val_f1, model)
            self.counter = 0
    
    def save_checkpoint(self, val_f1, model):
        '''validation F1ì´ ê°œì„ ë˜ë©´ ëª¨ë¸ ì €ì¥'''
        if self.verbose:
            print(f'âœ… Validation F1 improved ({self.best_f1:.4f} â†’ {val_f1:.4f}). Saving model...')
        self.best_model_state = copy.deepcopy(model.state_dict())
        self.best_f1 = val_f1


# ğŸ”¥ MPS í˜¸í™˜ì„±ì„ ìœ„í•œ ëª¨ë¸ ë˜í¼
class MPSCompatibleModel(nn.Module):
    """
    MPSì—ì„œ ë°œìƒí•˜ëŠ” view() í˜¸í™˜ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë˜í¼
    """
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
    
    def forward(self, x):
        # ì…ë ¥ì„ contiguousí•˜ê²Œ ë§Œë“¦
        x = x.contiguous()
        output = self.base_model(x)
        # ì¶œë ¥ë„ contiguousí•˜ê²Œ ë§Œë“¦
        return output.contiguous()
    
    def load_state_dict(self, state_dict, strict=True):
        return self.base_model.load_state_dict(state_dict, strict=strict)
    
    def state_dict(self):
        return self.base_model.state_dict()
def train_one_epoch(model, train_loader, criterion, optimizer, device):
    """
    í•œ ì—í­ ë™ì•ˆ ëª¨ë¸ í•™ìŠµ
    
    Args:
        model: í•™ìŠµí•  ëª¨ë¸
        train_loader: í•™ìŠµ ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        optimizer: ì˜µí‹°ë§ˆì´ì €
        device: ë””ë°”ì´ìŠ¤ (CPU/GPU/MPS)
    
    Returns:
        train_loss: í‰ê·  í•™ìŠµ ì†ì‹¤
        train_acc: í•™ìŠµ ì •í™•ë„ (%)
        train_f1: í•™ìŠµ F1 Score
    """
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    
    for images, labels in tqdm(train_loader, desc="Training", leave=False):
        images, labels = images.to(device), labels.to(device)
        
        # Forward pass
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # í†µê³„
        running_loss += loss.item() * images.size(0)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
    
    train_loss = running_loss / total
    train_acc = 100. * correct / total
    train_f1 = f1_score(all_labels, all_preds, average='macro')
    
    return train_loss, train_acc, train_f1


def validate(model, val_loader, criterion, device):
    """
    ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ í‰ê°€
    
    Args:
        model: í‰ê°€í•  ëª¨ë¸
        val_loader: ê²€ì¦ ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        device: ë””ë°”ì´ìŠ¤ (CPU/GPU/MPS)
    
    Returns:
        val_loss: í‰ê·  ê²€ì¦ ì†ì‹¤
        val_acc: ê²€ì¦ ì •í™•ë„ (%)
        val_f1: ê²€ì¦ F1 Score (macro)
        all_preds: ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼
        all_labels: ëª¨ë“  ì‹¤ì œ ë ˆì´ë¸”
    """
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in tqdm(val_loader, desc="Validation", leave=False):
            images, labels = images.to(device), labels.to(device)
            
            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            # í†µê³„
            running_loss += loss.item() * images.size(0)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    val_loss = running_loss / total
    val_acc = 100. * correct / total
    val_f1 = f1_score(all_labels, all_preds, average='macro')

    return val_loss, val_acc, val_f1, all_preds, all_labels


def run_kfold_training(model, train_dataset_raw, train_labels, train_transform, val_transform,
                       num_classes, device, config_dict, use_wandb=True):
    """
    K-Fold Cross Validation í•™ìŠµ ì‹¤í–‰

    Args:
        model: ê¸°ë³¸ ëª¨ë¸ (ê° foldë§ˆë‹¤ ìƒˆë¡œ ìƒì„±ë¨)
        train_dataset_raw: ì›ë³¸ í•™ìŠµ ë°ì´í„°ì…‹
        train_labels: í•™ìŠµ ë°ì´í„° ë ˆì´ë¸”
        train_transform: í•™ìŠµ ë°ì´í„° augmentation
        val_transform: ê²€ì¦ ë°ì´í„° transform
        num_classes: í´ë˜ìŠ¤ ìˆ˜
        device: ë””ë°”ì´ìŠ¤
        config_dict: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (EPOCHS, BATCH_SIZE, LR, N_FOLDS, PATIENCE ë“±)
        use_wandb: Wandb ë¡œê¹… ì—¬ë¶€

    Returns:
        fold_results: ê° foldë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    from sklearn.model_selection import StratifiedKFold
    from torch.utils.data import DataLoader
    import torch.nn as nn
    import torch.optim as optim
    import copy

    # Config ì¶”ì¶œ
    EPOCHS = config_dict.get('EPOCHS', 10)
    BATCH_SIZE = config_dict.get('BATCH_SIZE', 32)
    LR = config_dict.get('LR', 0.001)
    N_FOLDS = config_dict.get('N_FOLDS', 5)
    PATIENCE = config_dict.get('PATIENCE', 5)
    SELECTED_MODEL = config_dict.get('MODEL_NAME', 'efficientnet_b0')

    # K-Fold ì„¤ì •
    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)
    fold_results = []

    print("=" * 70)
    print("ğŸš€ K-Fold Cross Validation ì‹œì‘")
    print(f"Epochs: {EPOCHS}, Batch Size: {BATCH_SIZE}, Folds: {N_FOLDS}")
    print("=" * 70)

    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(train_labels)), train_labels)):
        print(f"\n{'='*70}")
        print(f"ğŸ“‚ Fold {fold + 1}/{N_FOLDS}")
        print(f"{'='*70}")

        # DataLoader ìƒì„±
        from src.data import CIFAR10Dataset
        train_subset = CIFAR10Dataset(
            data_dir=train_dataset_raw.data_dir,
            train=True,
            transform=train_transform,
            indices=train_idx.tolist()
        )
        val_subset = CIFAR10Dataset(
            data_dir=train_dataset_raw.data_dir,
            train=True,
            transform=val_transform,
            indices=val_idx.tolist()
        )

        train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
        val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

        # ëª¨ë¸ ì¬ìƒì„±
        import timm
        fold_model = timm.create_model(SELECTED_MODEL, pretrained=True, num_classes=num_classes)
        fold_model = fold_model.to(device)

        # Optimizer & Loss
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(fold_model.parameters(), lr=LR)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)
        early_stopping = EarlyStopping(patience=PATIENCE)

        # í•™ìŠµ ì´ë ¥ ì €ì¥
        history = {
            'train_loss': [], 'train_acc': [], 'train_f1': [],
            'val_loss': [], 'val_acc': [], 'val_f1': []
        }

        # í•™ìŠµ
        for epoch in range(EPOCHS):
            print(f"\nğŸ“ Epoch [{epoch+1}/{EPOCHS}]")

            train_loss, train_acc, train_f1 = train_one_epoch(fold_model, train_loader, criterion, optimizer, device)
            val_loss, val_acc, val_f1, _, _ = validate(fold_model, val_loader, criterion, device)

            scheduler.step(val_f1)

            # ì´ë ¥ ì €ì¥
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['train_f1'].append(train_f1)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            history['val_f1'].append(val_f1)

            print(f"Train - Loss: {train_loss:.4f} | Acc: {train_acc:.2f}% | F1: {train_f1:.4f}")
            print(f"Val   - Loss: {val_loss:.4f} | Acc: {val_acc:.2f}% | F1: {val_f1:.4f}")

            # Wandb ë¡œê¹…
            if use_wandb:
                import wandb
                wandb.log({
                    f"fold_{fold+1}/train_loss": train_loss,
                    f"fold_{fold+1}/train_acc": train_acc,
                    f"fold_{fold+1}/train_f1": train_f1,
                    f"fold_{fold+1}/val_loss": val_loss,
                    f"fold_{fold+1}/val_acc": val_acc,
                    f"fold_{fold+1}/val_f1": val_f1,
                    f"fold_{fold+1}/epoch": epoch + 1
                })

            early_stopping(val_f1, fold_model)
            if early_stopping.early_stop:
                print("ğŸ›‘ Early stopping!")
                break

        fold_results.append({
            'fold': fold + 1,
            'best_val_f1': early_stopping.best_f1,
            'best_model_state': early_stopping.best_model_state,
            'history': history
        })

        print(f"\nâœ… Fold {fold + 1} ì™„ë£Œ! Best Val F1: {early_stopping.best_f1:.4f}")

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“Š K-Fold ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    for result in fold_results:
        print(f"Fold {result['fold']}: Best Val F1 = {result['best_val_f1']:.4f}")

    avg_f1 = np.mean([r['best_val_f1'] for r in fold_results])
    std_f1 = np.std([r['best_val_f1'] for r in fold_results])
    print("=" * 70)
    print(f"ğŸ“ˆ í‰ê·  Validation F1: {avg_f1:.4f} Â± {std_f1:.4f}")
    print("=" * 70)

    if use_wandb:
        import wandb
        wandb.log({
            "cv/avg_val_f1": avg_f1,
            "cv/std_val_f1": std_f1
        })

    return fold_results

