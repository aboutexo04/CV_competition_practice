# src/data.py

import torch
from torch.utils.data import DataLoader, Dataset
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np
from pathlib import Path
import pickle
import random

# ============================================
# Augmentation í•¨ìˆ˜ë“¤
# ============================================

def get_albumentations_train(img_size=224):
    """ì¼ë°˜ ì´ë¯¸ì§€ìš© augmentation (CIFAR-10 ë“±)"""
    return A.Compose([
        A.Resize(img_size, img_size),
        
        # Geometric
        A.Rotate(limit=5, p=0.3),
        A.ShiftScaleRotate(
            shift_limit=0.05,
            scale_limit=0.05,
            rotate_limit=5,
            p=0.3
        ),
        A.Perspective(scale=0.05, p=0.3),
        
        # Color & Brightness
        A.RandomBrightnessContrast(
            brightness_limit=0.2,
            contrast_limit=0.2,
            p=0.5
        ),
        
        # Noise & Blur
        A.GaussNoise(p=0.2),
        A.Blur(blur_limit=3, p=0.2),
        
        # Normalize
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
        ToTensorV2()
    ])


try:
    from augraphy import (
        InkBleed, PaperFactory, DirtyDrum,
        Jpeg, Brightness, AugraphyPipeline
    )
    AUGRAPHY_AVAILABLE = True
except ImportError:
    AUGRAPHY_AVAILABLE = False


def get_augraphy_train(img_size=224):
    """ë¬¸ì„œ íŠ¹í™” augmentation (Augraphy)"""
    if not AUGRAPHY_AVAILABLE:
        print("âš ï¸  Augraphy not installed. Falling back to Albumentations.")
        return get_albumentations_train(img_size)
    
    # Augraphy íŒŒì´í”„ë¼ì¸ (ì´ì œ ë¹¨ê°„ ì¤„ ì•ˆ ë‚˜ì˜´!)
    ink_phase = [
        InkBleed(intensity_range=(0.1, 0.3), p=0.3),
    ]
    
    paper_phase = [
        PaperFactory(p=0.3),
        DirtyDrum(p=0.2),
    ]
    
    post_phase = [
        Jpeg(quality_range=(60, 95), p=0.3),
        Brightness(brightness_range=(0.95, 1.05), p=0.3),
    ]
    
    augraphy_pipeline = AugraphyPipeline(ink_phase, paper_phase, post_phase)
    
    return A.Compose([
        A.Lambda(image=lambda x, **kwargs: augraphy_pipeline.augment(x)["output"]),
        A.Resize(img_size, img_size),
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
        ToTensorV2()
    ])



def get_hybrid_train(img_size=224, augraphy_strength='light'):
    """Augraphy + Albumentations í˜¼í•©"""

    # ê°•ë„ë³„ í™•ë¥ 
    if augraphy_strength == 'light':
        ink_p, paper_p, post_p = 0.2, 0.2, 0.2
    elif augraphy_strength == 'medium':
        ink_p, paper_p, post_p = 0.4, 0.4, 0.3
    else:  # heavy
        ink_p, paper_p, post_p = 0.6, 0.5, 0.4
    
    # Augraphy íŒŒì´í”„ë¼ì¸
    ink_phase = [InkBleed(intensity_range=(0.05, 0.15), p=ink_p)]
    paper_phase = [PaperFactory(p=paper_p), DirtyDrum(p=paper_p * 0.5)]
    post_phase = [
        Jpeg(quality_range=(70, 95), p=post_p),
        Brightness(brightness_range=(0.95, 1.05), p=post_p)
    ]
    
    augraphy_pipeline = AugraphyPipeline(ink_phase, paper_phase, post_phase)
    
    return A.Compose([
        # Augraphy ì ìš©
        A.Lambda(image=lambda x, **kwargs: augraphy_pipeline.augment(x)["output"]),
        
        # ì¼ë°˜ augmentation
        A.Rotate(limit=3, p=0.4),
        A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.4),
        A.GaussNoise(var_limit=(5, 30), p=0.2),
        
        # ì „ì²˜ë¦¬
        A.Resize(img_size, img_size),
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
        ToTensorV2()
    ])


def get_val_augmentation(img_size=224):
    """ê²€ì¦ìš© augmentation (ë³€í™˜ ì—†ìŒ)"""
    return A.Compose([
        A.Resize(img_size, img_size),
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
        ToTensorV2()
    ])


def get_train_augmentation(config):
    """
    Config ê¸°ë°˜ìœ¼ë¡œ augmentation ìë™ ì„ íƒ
    
    Args:
        config: Config ê°ì²´
    """
    strategy = config.AUG_STRATEGY
    dataset_type = config.DATASET_TYPE
    img_size = config.IMAGE_SIZE
    
    # Auto ëª¨ë“œ: ë°ì´í„°ì…‹ì— ë§ê²Œ ìë™ ì„ íƒ
    if strategy == 'auto':
        if dataset_type in ['cifar10', 'cifar100', 'imagenet']:
            strategy = 'albumentations'
        elif dataset_type in ['document', 'text', 'ocr']:
            strategy = 'hybrid'
        else:
            strategy = 'albumentations'
        
        print(f"ğŸ“Œ Auto mode: {dataset_type} â†’ {strategy} augmentation")
    
    # ì „ëµë³„ augmentation
    if strategy == 'albumentations':
        return get_albumentations_train(img_size)
    elif strategy == 'augraphy':
        return get_augraphy_train(img_size)
    elif strategy == 'hybrid':
        augraphy_strength = getattr(config, 'AUGRAPHY_STRENGTH', 'light')
        return get_hybrid_train(img_size, augraphy_strength=augraphy_strength)
    else:
        raise ValueError(f"Unknown strategy: {strategy}")


# ============================================
# CIFAR10 Dataset
# ============================================

class CIFAR10Dataset(Dataset):
    """
    CIFAR10 ë°ì´í„°ì…‹ì„ pickle íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œ
    
    Args:
        data_dir: CIFAR10 ë°ì´í„°ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬
        train: Trueë©´ train ë°ì´í„°, Falseë©´ test ë°ì´í„°
        transform: Albumentations transform
        indices: ì‚¬ìš©í•  ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ì„œë¸Œìƒ˜í”Œë§ìš©, Noneì´ë©´ ì „ì²´)
    """
    def __init__(self, data_dir, train=True, transform=None, indices=None):
        self.data_dir = Path(data_dir)
        self.train = train
        self.transform = transform
        
        # CIFAR10 ë°ì´í„° ë¡œë“œ
        self.data = []
        self.labels = []
        
        if train:
            # Train ë°ì´í„°: data_batch_1 ~ data_batch_5
            for i in range(1, 6):
                batch_path = self.data_dir / f'data_batch_{i}'
                with open(batch_path, 'rb') as f:
                    batch = pickle.load(f, encoding='bytes')
                    labels = batch.get(b'labels', batch.get('labels', []))
                    batch_data = batch.get(b'data', batch.get('data', None))
                    if batch_data is not None:
                        self.data.append(batch_data)
                        self.labels.extend(labels)
            
            self.data = np.vstack(self.data)  # (50000, 3072)
        else:
            # Test ë°ì´í„°
            batch_path = self.data_dir / 'test_batch'
            with open(batch_path, 'rb') as f:
                batch = pickle.load(f, encoding='bytes')
                labels = batch.get(b'labels', batch.get('labels', []))
                batch_data = batch.get(b'data', batch.get('data', None))
                if batch_data is not None:
                    self.data = batch_data
                    self.labels = labels
        
        # ë°ì´í„° í˜•íƒœ ë³€í™˜ (3072 -> 32x32x3)
        self.data = self.data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)
        
        # íŠ¹ì • ì¸ë±ìŠ¤ë§Œ ì‚¬ìš© (ì„œë¸Œìƒ˜í”Œë§)
        if indices is not None:
            self.data = self.data[indices]
            self.labels = [self.labels[i] for i in indices]
            self.indices = indices
        else:
            self.indices = list(range(len(self.data)))
        
        self.labels = np.array(self.labels)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        image = self.data[idx]  # (32, 32, 3)
        label = int(self.labels[idx])
        
        # Transform ì ìš©
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        
        return image, label


# ============================================
# CIFAR10 ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# ============================================

def load_cifar10(config):
    """
    Config ê¸°ë°˜ìœ¼ë¡œ CIFAR10 ë°ì´í„° ë¡œë“œ
    
    Args:
        config: Config ê°ì²´
        
    Returns:
        train_dataset_raw, test_dataset, train_labels, class_names, num_classes
    """
    print("="*60)
    print("ğŸ“¦ Loading CIFAR10 Data")
    print("="*60)
    
    # ë°ì´í„° ê²½ë¡œ ì„¤ì •
    current_file = Path(__file__).resolve()
    project_root = current_file.parent.parent
    data_dir = project_root / 'data' / 'cifar-10-batches-py'
    
    # ì „ì²´ ë°ì´í„° ë¡œë“œ
    train_data_full = CIFAR10Dataset(
        data_dir=str(data_dir),
        train=True,
        transform=None
    )
    
    test_data_full = CIFAR10Dataset(
        data_dir=str(data_dir),
        train=False,
        transform=None
    )
    
    # í´ë˜ìŠ¤ ì´ë¦„ ë¡œë“œ
    meta_path = data_dir / 'batches.meta'
    with open(meta_path, 'rb') as f:
        meta = pickle.load(f, encoding='bytes')
        if b'label_names' in meta:
            class_names = [name.decode('utf-8') for name in meta[b'label_names']]
        else:
            class_names = [f'class_{i}' for i in range(10)]
    
    num_classes = len(class_names)
    
    print(f"\nâœ… CIFAR10 Full Data Loaded!")
    print(f"Train: {len(train_data_full):,} images")
    print(f"Test:  {len(test_data_full):,} images")
    print(f"Classes: {num_classes}")
    print(f"Class names: {class_names}")
    
    # ì„œë¸Œìƒ˜í”Œë§ (config ê¸°ë°˜)
    if config.USE_SUBSET:
        print(f"\nğŸ”¥ Subset mode: Using {int(config.SUBSET_RATIO*100)}% of data")
        
        # Train ì„œë¸Œìƒ˜í”Œë§
        train_labels_full = train_data_full.labels.tolist()
        train_indices = _stratified_subsample(
            train_labels_full, 
            ratio=config.SUBSET_RATIO
        )
        
        train_dataset_raw = CIFAR10Dataset(
            data_dir=str(data_dir),
            train=True,
            transform=None,
            indices=train_indices
        )
        train_labels = [train_labels_full[i] for i in train_indices]
        
        # Test ì„œë¸Œìƒ˜í”Œë§
        test_labels_full = test_data_full.labels.tolist()
        test_indices = _stratified_subsample(
            test_labels_full,
            ratio=config.SUBSET_RATIO
        )
        
        test_dataset = CIFAR10Dataset(
            data_dir=str(data_dir),
            train=False,
            transform=get_val_augmentation(config.IMAGE_SIZE),
            indices=test_indices
        )
        
        print(f"âœ… Subset train size: {len(train_dataset_raw):,}")
        print(f"âœ… Subset test size: {len(test_dataset):,}")
    else:
        train_dataset_raw = train_data_full
        train_labels = train_data_full.labels.tolist()
        test_dataset = CIFAR10Dataset(
            data_dir=str(data_dir),
            train=False,
            transform=get_val_augmentation(config.IMAGE_SIZE)
        )
    
    return train_dataset_raw, test_dataset, train_labels, class_names, num_classes


def _stratified_subsample(labels, ratio):
    """í´ë˜ìŠ¤ë³„ ê· ë“± ì„œë¸Œìƒ˜í”Œë§"""
    indices_by_class = {}
    for idx, label in enumerate(labels):
        if label not in indices_by_class:
            indices_by_class[label] = []
        indices_by_class[label].append(idx)
    
    selected_indices = []
    for label, indices in indices_by_class.items():
        n_samples = int(len(indices) * ratio)
        selected_indices.extend(random.sample(indices, n_samples))
    
    selected_indices.sort()
    return selected_indices


# ============================================
# DataLoader ìƒì„±
# ============================================

def get_dataloaders(train_dataset_raw, train_labels, test_dataset, config):
    """
    Config ê¸°ë°˜ìœ¼ë¡œ DataLoader ìƒì„±
    
    Args:
        train_dataset_raw: Transform ì—†ëŠ” train dataset
        train_labels: Train labels (K-Foldìš©)
        test_dataset: Transform ì ìš©ëœ test dataset
        config: Config ê°ì²´
        
    Returns:
        train_loader, val_loader (for single split)
        ë˜ëŠ” K-Foldì—ì„œ ì§ì ‘ ì‚¬ìš©
    """
    # ì´ í•¨ìˆ˜ëŠ” K-Foldì—ì„œ ì§ì ‘ ì‚¬ìš©ë˜ë¯€ë¡œ
    # ì—¬ê¸°ì„œëŠ” test_loaderë§Œ ìƒì„±
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=0  # MPS í˜¸í™˜ì„±
    )
    
    return test_loader


# ============================================
# ì „ì—­ ë³€ìˆ˜ë¡œ ë°ì´í„° ë¡œë“œ (backward compatibility)
# ============================================

# Config import
# try:
#     from .config import config
    
#     # ë°ì´í„° ë¡œë“œ
#     train_dataset_raw, test_dataset, train_labels, class_names, num_classes = load_cifar10(config)
    
#     # Device ì„¤ì • (configì—ì„œ)
#     device = config.DEVICE
    
#     # Augmentation
#     train_transform = get_train_augmentation(config)
#     val_transform = get_val_augmentation(config.IMAGE_SIZE)
    
#     print(f"\nğŸ–¥ï¸  Device: {device}")
#     print("="*60)
    
# except ImportError:
#     print("âš ï¸  Config not found. Please import manually.")

# src/data.py ë§¨ ëì— ì¶”ê°€ (ê¸°ì¡´ ì½”ë“œ ë’¤ì—)

# ============================================
# í†µí•© ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# ============================================

def load_data(config):
    """
    Config ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ìë™ ë¡œë“œ
    
    Args:
        config: Config ê°ì²´
        
    Returns:
        train_dataset_raw, test_dataset, train_labels, class_names, num_classes
    """
    print(f"\nğŸ¯ Dataset Type: {config.DATASET_TYPE}")
    
    if config.DATASET_TYPE == 'cifar10':
        return load_cifar10(config)
    elif config.DATASET_TYPE == 'cifar100':
        return load_cifar100(config)  # ë‚˜ì¤‘ì— í•„ìš”í•˜ë©´ êµ¬í˜„
    elif config.DATASET_TYPE == 'document':
        return load_document_data(config)
    else:
        raise ValueError(
            f"Unknown dataset type: {config.DATASET_TYPE}\n"
            f"Available types: 'cifar10', 'document'"
        )


def load_document_data(config):
    """
    ë¬¸ì„œ ë¶„ë¥˜ ëŒ€íšŒ ë°ì´í„° ë¡œë“œ
    
    Args:
        config: Config ê°ì²´
        
    Returns:
        train_dataset_raw, test_dataset, train_labels, class_names, num_classes
    """
    print("="*60)
    print("ğŸ“„ Loading Document Classification Data")
    print("="*60)
    
    # ë°ì´í„° ê²½ë¡œ
    current_file = Path(__file__).resolve()
    project_root = current_file.parent.parent
    data_dir = project_root / 'data' / 'document_competition'
    
    if not data_dir.exists():
        raise FileNotFoundError(
            f"âŒ Document data not found at {data_dir}\n"
            f"Please download competition data first."
        )
    
    # TODO: ëŒ€íšŒ ì‹œì‘í•˜ë©´ ì•„ë˜ êµ¬í˜„
    # 
    # ì˜ˆì‹œ êµ¬ì¡°:
    # 
    # class DocumentDataset(Dataset):
    #     def __init__(self, data_dir, transform=None):
    #         # CSV ë˜ëŠ” ì´ë¯¸ì§€ í´ë”ì—ì„œ ë¡œë“œ
    #         self.image_paths = list(data_dir.glob('*.jpg'))
    #         self.labels = pd.read_csv(data_dir / 'labels.csv')
    #         self.transform = transform
    #     
    #     def __getitem__(self, idx):
    #         img = cv2.imread(str(self.image_paths[idx]))
    #         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    #         label = self.labels.iloc[idx]['label']
    #         
    #         if self.transform:
    #             augmented = self.transform(image=img)
    #             img = augmented['image']
    #         
    #         return img, label
    # 
    # train_dataset_raw = DocumentDataset(
    #     data_dir / 'train',
    #     transform=None
    # )
    # 
    # test_dataset = DocumentDataset(
    #     data_dir / 'test',
    #     transform=get_val_augmentation(config.IMAGE_SIZE)
    # )
    # 
    # train_labels = train_dataset_raw.labels.tolist()
    # class_names = ['class_0', 'class_1', ...]  # ëŒ€íšŒ ê³µì§€ ì°¸ê³ 
    # num_classes = len(class_names)
    # 
    # # ì„œë¸Œìƒ˜í”Œë§ (í•„ìš”ì‹œ)
    # if config.USE_SUBSET:
    #     train_indices = _stratified_subsample(train_labels, config.SUBSET_RATIO)
    #     train_dataset_raw = Subset(train_dataset_raw, train_indices)
    #     train_labels = [train_labels[i] for i in train_indices]
    # 
    # return train_dataset_raw, test_dataset, train_labels, class_names, num_classes
    
    raise NotImplementedError(
        "ğŸ“ Document dataset loader not implemented yet.\n"
        "Implement this function when competition data is available.\n"
    )
def load_cifar100(config):
    """CIFAR-100 ë¡œë” (í•„ìš”ì‹œ êµ¬í˜„)"""
    raise NotImplementedError("CIFAR-100 loader not implemented yet.")
# ============================================
# ì „ì—­ ë³€ìˆ˜ë¡œ ë°ì´í„° ë¡œë“œ (backward compatibility)
# ============================================

# try:
#     from .config import config
    
#     # load_data() ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½! â­
#     train_dataset_raw, test_dataset, train_labels, class_names, num_classes = load_data(config)
    
#     device = config.DEVICE
#     train_transform = get_train_augmentation(config)
#     val_transform = get_val_augmentation(config.IMAGE_SIZE)
    
#     print(f"\nğŸ–¥ï¸  Device: {device}")
#     print("="*60)
    
# except ImportError:
#     print("âš ï¸  Config not found. Please import manually.")
# except NotImplementedError as e:
#     print(f"âš ï¸  {e}")