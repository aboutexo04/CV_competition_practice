# src/data.py

import torch
from torch.utils.data import DataLoader, Dataset
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np
from pathlib import Path
import pandas as pd
from PIL import Image
from src.config import config

# ============================================
# Augmentation í•¨ìˆ˜ë“¤
# ============================================

def get_albumentations_train(image_size):
    """ì¼ë°˜ ì´ë¯¸ì§€ìš© augmentation - ê°•í™” ë²„ì „"""

    return A.Compose([
        A.Resize(image_size, image_size),

        # ê¸°í•˜í•™ì  ë³€í™˜ ê°•í™”
        A.HorizontalFlip(p=0.5),
        A.Rotate(limit=10, p=0.5),
        A.ShiftScaleRotate(
            shift_limit=0.1,
            scale_limit=0.15,
            rotate_limit=10,
            p=0.5
        ),

        # ìƒ‰ìƒ/ëª…ì•” ë³€í™˜ ì¶”ê°€
        A.RandomBrightnessContrast(
            brightness_limit=0.2,
            contrast_limit=0.2,
            p=0.5
        ),
        A.HueSaturationValue(
            hue_shift_limit=10,
            sat_shift_limit=20,
            val_shift_limit=15,
            p=0.4
        ),

        # ë…¸ì´ì¦ˆ ë° ë¸”ëŸ¬ ì¶”ê°€
        A.OneOf([
            A.GaussNoise(var_limit=(10, 50), p=1.0),
            A.GaussianBlur(blur_limit=(3, 5), p=1.0),
            A.MotionBlur(blur_limit=5, p=1.0),
        ], p=0.3),

        # í’ˆì§ˆ ì €í•˜ ì‹œë®¬ë ˆì´ì…˜
        A.OneOf([
            A.ImageCompression(quality_lower=70, quality_upper=95, p=1.0),
            A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1.0),
        ], p=0.3),

        # ì •ê·œí™”
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
        ToTensorV2()
    ])


try:
    from augraphy import (
        InkBleed, PaperFactory, DirtyDrum,
        Jpeg, Brightness, AugraphyPipeline
    )
    AUGRAPHY_AVAILABLE = True
except ImportError:
    AUGRAPHY_AVAILABLE = False


def get_augraphy_train(image_size):  # âœ… íŒŒë¼ë¯¸í„°ëª… í†µì¼
    """ë¬¸ì„œ íŠ¹í™” augmentation (Augraphy)"""
    if not AUGRAPHY_AVAILABLE:
        print("âš ï¸  Augraphy not installed. Falling back to Albumentations.")
        return get_albumentations_train(image_size)  # âœ… ìˆ˜ì •
    
    import numpy as np
    
    # Augraphy íŒŒì´í”„ë¼ì¸
    ink_phase = [
        InkBleed(intensity_range=(0.1, 0.3), p=0.2),
    ]
    
    paper_phase = [
        PaperFactory(p=0.2),
        DirtyDrum(p=0.1),
    ]
    
    post_phase = [
        Jpeg(quality_range=(60, 95), p=0.2),
        Brightness(brightness_range=(0.95, 1.05), p=0.2),
    ]
    
    augraphy_pipeline = AugraphyPipeline(ink_phase, paper_phase, post_phase)
    
    # âœ… Augraphy ì ìš© + ì±„ë„ ë³´ì • í•¨ìˆ˜
    def apply_augraphy_safe(image, **kwargs):
        """Augraphy ì ìš© í›„ RGB ì±„ë„ ë³´ì¥"""
        # Augraphy ì ìš©
        result = augraphy_pipeline.augment(image)["output"]
        
        # ì±„ë„ ìˆ˜ í™•ì¸ ë° ë³´ì •
        if len(result.shape) == 2:  # Grayscale (H, W)
            result = np.stack([result] * 3, axis=-1)  # (H, W, 3)
        elif result.shape[-1] == 1:  # (H, W, 1)
            result = np.repeat(result, 3, axis=-1)  # (H, W, 3)
        elif result.shape[-1] == 4:  # RGBA (H, W, 4)
            result = result[:, :, :3]  # RGBë§Œ (H, W, 3)
        
        return result
    
    return A.Compose([
        A.Lambda(image=apply_augraphy_safe),
        A.Resize(image_size, image_size),  # âœ… ìˆ˜ì •
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
        ToTensorV2()
    ])


def get_hybrid_train(image_size, augraphy_strength='light'):  # âœ… íŒŒë¼ë¯¸í„°ëª… í†µì¼
    """Augraphy + Albumentations í˜¼í•©"""
    
    if not AUGRAPHY_AVAILABLE:
        print("âš ï¸  Augraphy not installed. Falling back to Albumentations.")
        return get_albumentations_train(image_size)  # âœ… ìˆ˜ì •

    import numpy as np

    # ê°•ë„ë³„ í™•ë¥ 
    if augraphy_strength == 'light':
        ink_p, paper_p, post_p = 0.2, 0.2, 0.2
    elif augraphy_strength == 'medium':
        ink_p, paper_p, post_p = 0.4, 0.4, 0.3
    elif augraphy_strength == 'heavy':  # heavy
        ink_p, paper_p, post_p = 0.6, 0.5, 0.4
    
    # Augraphy íŒŒì´í”„ë¼ì¸
    ink_phase = [InkBleed(intensity_range=(0.05, 0.15), p=ink_p)]
    paper_phase = [PaperFactory(p=paper_p), DirtyDrum(p=paper_p * 0.5)]
    post_phase = [
        Jpeg(quality_range=(70, 95), p=post_p),
        Brightness(brightness_range=(0.95, 1.05), p=post_p)
    ]
    
    augraphy_pipeline = AugraphyPipeline(ink_phase, paper_phase, post_phase)
    
    # âœ… ì±„ë„ ë³´ì • í•¨ìˆ˜
    def apply_augraphy_safe(image, **kwargs):
        result = augraphy_pipeline.augment(image)["output"]
        
        if len(result.shape) == 2:
            result = np.stack([result] * 3, axis=-1)
        elif result.shape[-1] == 1:
            result = np.repeat(result, 3, axis=-1)
        elif result.shape[-1] == 4:
            result = result[:, :, :3]
        
        return result
    
    return A.Compose([
        # Augraphy ì ìš©
        A.Lambda(image=apply_augraphy_safe),

        # ì¼ë°˜ augmentation
        A.Rotate(limit=3, p=0.4),
        A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.4),
        A.GaussNoise(var_limit=(5, 30), p=0.2),

        # ì „ì²˜ë¦¬
        A.Resize(image_size, image_size),
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
        ToTensorV2()
    ])


def get_val_augmentation(image_size):  # âœ… íŒŒë¼ë¯¸í„°ëª… í†µì¼
    """ê²€ì¦ìš© augmentation (ë³€í™˜ ì—†ìŒ)"""
    return A.Compose([
        A.Resize(image_size, image_size),  # âœ… ìˆ˜ì •
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
        ToTensorV2()
    ])


def get_train_augmentation(image_size, config):  # âœ… ì‹œê·¸ë‹ˆì²˜ ìˆ˜ì •
    """
    Config ê¸°ë°˜ìœ¼ë¡œ augmentation ìë™ ì„ íƒ
    
    Args:
        image_size: ì´ë¯¸ì§€ í¬ê¸°
        config: Config ê°ì²´
    """
    # Configì—ì„œ ì†ì„± ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
    strategy = getattr(config, 'AUG_STRATEGY', 'auto')
    dataset_type = getattr(config, 'DATASET_TYPE', 'document')
    
    # Auto ëª¨ë“œ: ë°ì´í„°ì…‹ì— ë§ê²Œ ìë™ ì„ íƒ
    if strategy == 'auto':
        if dataset_type in ['document', 'text', 'ocr']:
            strategy = 'hybrid'  # ë¬¸ì„œëŠ” hybrid (Augraphy + Albumentations)
        else:
            strategy = 'albumentations'  # ì¼ë°˜ ì´ë¯¸ì§€

        print(f"ğŸ“Œ Auto mode: {dataset_type} â†’ {strategy} augmentation")
    
    # ì „ëµë³„ augmentation
    if strategy == 'albumentations':
        return get_albumentations_train(image_size)
    elif strategy == 'augraphy':
        return get_augraphy_train(image_size)
    elif strategy == 'hybrid':
        augraphy_strength = getattr(config, 'AUGRAPHY_STRENGTH', 'light')
        return get_hybrid_train(image_size, augraphy_strength=augraphy_strength)
    else:
        raise ValueError(f"Unknown strategy: {strategy}")


# ============================================
# Document Dataset í´ë˜ìŠ¤
# ============================================

class DocumentDataset(Dataset):
    """
    ë¬¸ì„œ ë¶„ë¥˜ ëŒ€íšŒìš© Dataset
    
    Args:
        df: train.csv DataFrame (ID, target ì»¬ëŸ¼)
        img_dir: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        transform: Albumentations transform
        indices: ì‚¬ìš©í•  ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ (K-Foldìš©, Noneì´ë©´ ì „ì²´)
    """
    def __init__(self, df, img_dir, transform=None, indices=None):
        self.img_dir = Path(img_dir)
        self.transform = transform
        
        if indices is not None:
            self.df = df.iloc[indices].reset_index(drop=True)
            self.indices = indices
        else:
            self.df = df
            self.indices = list(range(len(df)))
        
        self.image_ids = self.df['ID'].tolist()
        self.labels = self.df['target'].tolist()
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ë¡œë“œ
        img_id = self.image_ids[idx]
        img_path = self.img_dir / img_id
        
        try:
            image = Image.open(img_path).convert('RGB')
            image = np.array(image)
        except Exception as e:
            print(f"âš ï¸  Error loading image {img_path}: {e}")
            image = np.zeros((224, 224, 3), dtype=np.uint8)
        
        label = int(self.labels[idx])
        
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        
        return image, label


# ============================================
# DataLoader ìƒì„±
# ============================================

def get_dataloaders(train_dataset_raw, train_labels, test_dataset, config):
    """
    Config ê¸°ë°˜ìœ¼ë¡œ DataLoader ìƒì„±
    
    Args:
        train_dataset_raw: Transform ì—†ëŠ” train dataset
        train_labels: Train labels (K-Foldìš©)
        test_dataset: Transform ì ìš©ëœ test dataset
        config: Config ê°ì²´
        
    Returns:
        test_loader (K-Foldì—ì„œ ì§ì ‘ train_loader ìƒì„±)
    """
    # test_datasetì´ Noneì´ë©´ (ëŒ€íšŒ ì´ˆê¸°) None ë°˜í™˜
    if test_dataset is None:
        return None
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=0  # MPS í˜¸í™˜ì„±
    )
    
    return test_loader


# ============================================
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# ============================================

def load_document_data(config):
    """
    ë¬¸ì„œ ë¶„ë¥˜ ëŒ€íšŒ ë°ì´í„° ë¡œë“œ

    ë°ì´í„° êµ¬ì¡°:
    - train.csv: ID, target (ì´ë¯¸ì§€ëª… <-> í´ë˜ìŠ¤ ì¸ë±ìŠ¤)
    - meta.csv: target, class_name (í´ë˜ìŠ¤ ì¸ë±ìŠ¤ <-> í´ë˜ìŠ¤ëª…)
    - train/: í•™ìŠµ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬

    Args:
        config: Config ê°ì²´

    Returns:
        train_dataset_raw, test_dataset, train_labels, class_names, num_classes
    """
    print("="*60)
    print("ğŸ“„ Loading Document Classification Data")
    print("="*60)

    # Configì—ì„œ ë°ì´í„° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    train_csv_path = Path(config.TRAIN_CSV)
    meta_csv_path = Path(config.META_CSV)
    train_img_dir = Path(config.DATA_DIR) / 'train'
    test_img_dir = Path(config.TEST_DIR)
    
    # CSV íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not train_csv_path.exists():
        raise FileNotFoundError(
            f"âŒ train.csv not found at {train_csv_path}\n"
            f"Please check config.TRAIN_CSV path"
        )

    if not meta_csv_path.exists():
        raise FileNotFoundError(
            f"âŒ meta.csv not found at {meta_csv_path}\n"
            f"Please check config.META_CSV path"
        )
    
    # CSV ë¡œë“œ
    train_df = pd.read_csv(train_csv_path)
    meta_df = pd.read_csv(meta_csv_path)
    
    # í´ë˜ìŠ¤ ì •ë³´ ì¶”ì¶œ
    meta_df = meta_df.sort_values('target').reset_index(drop=True)
    class_names = meta_df['class_name'].tolist()
    num_classes = len(class_names)
    
    print(f"\nâœ… Document Data Loaded!")
    print(f"Total samples: {len(train_df):,}")
    print(f"Number of classes: {num_classes}")
    print(f"Class names: {class_names[:5]}{'...' if num_classes > 5 else ''}")
    
    # Dataset ìƒì„±
    train_dataset_raw = DocumentDataset(
        df=train_df,
        img_dir=train_img_dir,
        transform=None
    )
    train_labels = train_df['target'].tolist()

    # Test ë°ì´í„°ì…‹ ë¡œë“œ (test ë””ë ‰í† ë¦¬ê°€ ìˆëŠ” ê²½ìš°)
    if test_img_dir.exists():
        # test ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        test_image_files = sorted([f.name for f in test_img_dir.glob('*.jpg')])

        # testìš© DataFrame ìƒì„± (IDë§Œ ìˆê³  targetì€ ì—†ìŒ)
        test_df = pd.DataFrame({'ID': test_image_files})
        test_df['target'] = -1  # placeholder

        # Test dataset ìƒì„± (validation transform ì‚¬ìš©)
        test_transform = get_val_augmentation(config.IMAGE_SIZE)

        # Testìš© DocumentDataset (transform ì ìš©)
        class DocumentTestDataset(Dataset):
            def __init__(self, df, img_dir, transform):
                self.img_dir = Path(img_dir)
                self.transform = transform
                self.image_ids = df['ID'].tolist()

            def __len__(self):
                return len(self.image_ids)

            def __getitem__(self, idx):
                img_id = self.image_ids[idx]
                img_path = self.img_dir / img_id

                try:
                    image = Image.open(img_path).convert('RGB')
                    image = np.array(image)
                except Exception as e:
                    print(f"âš ï¸  Error loading image {img_path}: {e}")
                    image = np.zeros((224, 224, 3), dtype=np.uint8)

                if self.transform:
                    augmented = self.transform(image=image)
                    image = augmented['image']

                return image, -1  # labelì€ -1 (placeholder)

        test_dataset = DocumentTestDataset(
            df=test_df,
            img_dir=test_img_dir,
            transform=test_transform
        )

        print(f"Test samples: {len(test_dataset):,}")
    else:
        test_dataset = None
        print("Test directory not found - test_dataset set to None")

    return train_dataset_raw, test_dataset, train_labels, class_names, num_classes


def load_data(config):
    """
    Config ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ìë™ ë¡œë“œ
    
    Args:
        config: Config ê°ì²´
        
    Returns:
        train_dataset_raw, test_dataset, train_labels, class_names, num_classes
    """
    print(f"\nğŸ¯ Dataset Type: {getattr(config, 'DATASET_TYPE', 'document')}")
    
    dataset_type = getattr(config, 'DATASET_TYPE', 'document')
    
    if dataset_type == 'document':
        return load_document_data(config)
    else:
        raise ValueError(
            f"Unknown dataset type: {dataset_type}\n"
            f"Available types: 'document'"
        )