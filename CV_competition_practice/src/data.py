import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, Subset
import torchvision
from torchvision import datasets
import torchvision.transforms as transforms
import timm
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, classification_report, confusion_matrix
import seaborn as sns
import copy
from pathlib import Path
import pickle
import albumentations as A
from albumentations.pytorch import ToTensorV2
import random

# configì—ì„œ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸° (ì˜µì…˜)
try:
    from .config import SELECTED_MODEL, IMAGE_SIZE
except ImportError:
    # configë¥¼ ëª»ì°¾ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
    SELECTED_MODEL = 'efficientnet_b0'
    IMAGE_SIZE = 224

# ë””ë°”ì´ìŠ¤ ì„¤ì •
# ğŸ”¥ ConvNeXtì™€ ì¼ë¶€ ëª¨ë¸ì€ MPSì—ì„œ view() ë¬¸ì œê°€ ìˆì–´ CPU ì‚¬ìš©
PROBLEMATIC_MODELS = ['convnext_tiny', 'convnext_small', 'convnext_base']

if SELECTED_MODEL in PROBLEMATIC_MODELS:
    device = torch.device("cpu")
    print(f'Using device: CPU (MPS has compatibility issues with {SELECTED_MODEL})')
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    print(f'Using device: MPS (Apple Silicon)')
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print(f'Using device: CUDA')
else:
    device = torch.device("cpu")
    print(f'Using device: CPU')

# ============================================
# Train Transform - 100% ì‘ë™ ë³´ì¥! â­â­â­â­â­
# ============================================
train_transform = A.Compose([
    # í•„ìˆ˜: ë¦¬ì‚¬ì´ì¦ˆ
    A.Resize(IMAGE_SIZE, IMAGE_SIZE),
    
    # ë¬¸ì„œ íŠ¹í™” Augmentation
    A.Rotate(limit=5, p=0.3),  # ì‚´ì§ íšŒì „
    A.Perspective(scale=0.05, p=0.3),  # ê°ë„ ë³€í™”
    
    # ë…¸ì´ì¦ˆ ì¶”ê°€ (ìŠ¤ìº” íš¨ê³¼)
    A.GaussNoise(p=0.2),
    
    # íë¦¼ íš¨ê³¼ (ì••ì¶•/ìŠ¤ìº” íš¨ê³¼)
    A.Blur(blur_limit=3, p=0.2),
    
    # ë°ê¸°/ëŒ€ë¹„ ì¡°ì • (ë§¤ìš° ì¤‘ìš”!)
    A.RandomBrightnessContrast(
        brightness_limit=0.2,
        contrast_limit=0.2,
        p=0.5
    ),
    
    # ì„ íƒ: ì¶”ê°€ Augmentation
    A.ShiftScaleRotate(
        shift_limit=0.05,
        scale_limit=0.05,
        rotate_limit=5,
        p=0.3
    ),
    
    # Normalize (í•„ìˆ˜!)
    A.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
    
    ToTensorV2()
])

# ============================================
# Validation Transform
# ============================================
val_transform = A.Compose([
    A.Resize(IMAGE_SIZE, IMAGE_SIZE),
    A.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
    ToTensorV2()
])

# ============================================
# CIFAR10 ë°ì´í„° ì§ì ‘ ë¡œë”© (pickle íŒŒì¼ì—ì„œ)
# ============================================


class CIFAR10Dataset(Dataset):
    """
    CIFAR10 ë°ì´í„°ì…‹ì„ pickle íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œ
    
    Args:
        data_dir: CIFAR10 ë°ì´í„°ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ (cifar-10-batches-py í´ë”)
        train: Trueë©´ train ë°ì´í„°, Falseë©´ test ë°ì´í„°
        transform: Albumentations transform
        indices: ì‚¬ìš©í•  ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ì„œë¸Œìƒ˜í”Œë§ìš©, Noneì´ë©´ ì „ì²´)
    """
    def __init__(self, data_dir, train=True, transform=None, indices=None):
        self.data_dir = Path(data_dir)
        self.train = train
        self.transform = transform
        
        # CIFAR10 ë°ì´í„° ë¡œë“œ
        self.data = []
        self.labels = []
        
        if train:
            # Train ë°ì´í„°: data_batch_1 ~ data_batch_5
            for i in range(1, 6):
                batch_path = self.data_dir / f'data_batch_{i}'
                with open(batch_path, 'rb') as f:
                    batch = pickle.load(f, encoding='bytes')
                    # CIFAR10ì€ í•­ìƒ bytes í‚¤ë¥¼ ì‚¬ìš©
                    labels = batch.get(b'labels', batch.get('labels', []))
                    batch_data = batch.get(b'data', batch.get('data', None))
                    if batch_data is not None:
                        self.data.append(batch_data)
                        self.labels.extend(labels)
                    else:
                        raise ValueError(f"CIFAR10 data_batch_{i} file is missing 'data' key")
            
            self.data = np.vstack(self.data)  # (50000, 3072)
        else:
            # Test ë°ì´í„°: test_batch
            batch_path = self.data_dir / 'test_batch'
            with open(batch_path, 'rb') as f:
                batch = pickle.load(f, encoding='bytes')
                # CIFAR10ì€ í•­ìƒ bytes í‚¤ë¥¼ ì‚¬ìš©
                # labels ì²˜ë¦¬
                labels = batch.get(b'labels', batch.get('labels', []))
                
                # data ì²˜ë¦¬
                batch_data = batch.get(b'data', batch.get('data', None))
                if batch_data is not None:
                    self.data = batch_data
                    self.labels = labels
                else:
                    raise ValueError(f"CIFAR10 test batch file is missing 'data' key")
        
        # ë°ì´í„° í˜•íƒœ ë³€í™˜ (3072 -> 32x32x3)
        self.data = self.data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # (N, 32, 32, 3)
        
        # íŠ¹ì • ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
        if indices is not None:
            self.data = self.data[indices]
            self.labels = [self.labels[i] for i in indices]
            self.indices = indices
        else:
            self.indices = list(range(len(self.data)))
        
        self.labels = np.array(self.labels)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ numpy array)
        image = self.data[idx]  # (32, 32, 3)
        
        # ë ˆì´ë¸” ê°€ì ¸ì˜¤ê¸°
        label = int(self.labels[idx])
        
        # Transform ì ìš©
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        
        return image, label
# ============================================
# CIFAR10 ë°ì´í„° ë¡œë”© (pickle íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œ)
# ============================================

print("="*60)
print("ğŸ“¦ Loading CIFAR10 Data (from pickle files)")
print("="*60)

# ë°ì´í„° ê²½ë¡œ ì„¤ì • (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
# í˜„ì¬ íŒŒì¼(data.py)ì˜ ìœ„ì¹˜ì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì•„ì„œ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent
data_dir = project_root / 'data' / 'cifar-10-batches-py'

# CIFAR10 ë°ì´í„° ì§ì ‘ ë¡œë“œ
print("âœ… Loading CIFAR10 data from pickle files...")
train_data = CIFAR10Dataset(
    data_dir=str(data_dir),
    train=True,
    transform=None  # ë‚˜ì¤‘ì— transform ì ìš©
)

test_data = CIFAR10Dataset(
    data_dir=str(data_dir),
    train=False,
    transform=None
)

# í´ë˜ìŠ¤ ì´ë¦„ ë¡œë“œ
meta_path = data_dir / 'batches.meta'
with open(meta_path, 'rb') as f:
    meta = pickle.load(f, encoding='bytes')
    if b'label_names' in meta:
        class_names = [name.decode('utf-8') for name in meta[b'label_names']]
    else:
        class_names = meta.get('label_names', [f'class_{i}' for i in range(10)])

num_classes = len(class_names)

print(f"\nâœ… CIFAR10 Data Loaded!")
print(f"Train: {len(train_data):,} images")
print(f"Test:  {len(test_data):,} images")
print(f"Classes: {num_classes}")
print(f"Class names: {class_names}")
print(f"Location: {data_dir.absolute()}")

import random

# ğŸ”¥ ì—°ìŠµìš© ì„¤ì •: ë°ì´í„°ì…‹ í¬ê¸° ì¡°ì ˆ (10% ì‚¬ìš©)
USE_SUBSET = True  # Falseë¡œ ë³€ê²½í•˜ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©
SUBSET_RATIO = 0.1  # 10%ë§Œ ì‚¬ìš© (0.1 ~ 1.0)

# train_dataì™€ test_dataëŠ” ì´ë¯¸ CIFAR10Datasetìœ¼ë¡œ ë¡œë“œë¨
# ë ˆì´ë¸” ì¶”ì¶œ (K-Foldìš©)
train_labels = train_data.labels.tolist()

# ğŸ”¥ ë°ì´í„°ì…‹ ì„œë¸Œìƒ˜í”Œë§ (í´ë˜ìŠ¤ë³„ ê· ë“± ìƒ˜í”Œë§)
if USE_SUBSET:
    # Train ë°ì´í„° ì„œë¸Œìƒ˜í”Œë§
    train_indices_by_class = {}
    for idx, label in enumerate(train_labels):
        if label not in train_indices_by_class:
            train_indices_by_class[label] = []
        train_indices_by_class[label].append(idx)
    
    # ê° í´ë˜ìŠ¤ì—ì„œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
    selected_train_indices = []
    for label, indices in train_indices_by_class.items():
        n_samples = int(len(indices) * SUBSET_RATIO)
        selected_train_indices.extend(random.sample(indices, n_samples))
    
    selected_train_indices.sort()
    train_dataset_raw = CIFAR10Dataset(
        data_dir=train_data.data_dir,
        train=True,
        transform=None,
        indices=selected_train_indices
    )
    train_labels = [train_labels[i] for i in selected_train_indices]
    
    # Test ë°ì´í„° ì„œë¸Œìƒ˜í”Œë§
    test_labels = test_data.labels.tolist()
    test_indices_by_class = {}
    for idx, label in enumerate(test_labels):
        if label not in test_indices_by_class:
            test_indices_by_class[label] = []
        test_indices_by_class[label].append(idx)
    
    selected_test_indices = []
    for label, indices in test_indices_by_class.items():
        n_samples = int(len(indices) * SUBSET_RATIO)
        selected_test_indices.extend(random.sample(indices, n_samples))
    
    selected_test_indices.sort()
    test_dataset_raw = CIFAR10Dataset(
        data_dir=test_data.data_dir,
        train=False,
        transform=None,  # ë‚˜ì¤‘ì— K-Foldì—ì„œ ì ìš©
        indices=selected_test_indices
    )
    
    print(f"ğŸ”¥ ë°œì—´ ê°ì†Œ ëª¨ë“œ: ë°ì´í„°ì…‹ {int(SUBSET_RATIO*100)}% ì‚¬ìš©")
else:
    train_dataset_raw = train_data
    test_dataset_raw = test_data

# Transform ì ìš©ëœ test dataset
# USE_SUBSETì¼ ë•ŒëŠ” ì„œë¸Œìƒ˜í”Œë§ëœ ê²ƒ ì‚¬ìš©, ì•„ë‹ˆë©´ ì „ì²´ ì‚¬ìš©
if USE_SUBSET:
    # ì„œë¸Œìƒ˜í”Œë§ëœ test_dataset_rawë¥¼ transformê³¼ í•¨ê»˜ ë‹¤ì‹œ ìƒì„±
    # test_dataset_rawê°€ ì´ë¯¸ indicesë¡œ ìƒì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, transformë§Œ ì ìš©
    if 'val_transform' in globals() and val_transform is not None:
        test_dataset = CIFAR10Dataset(
            data_dir=test_data.data_dir,
            train=False,
            transform=val_transform,
            indices=selected_test_indices if USE_SUBSET else None
        )
    else:
        # val_transformì´ ì—†ìœ¼ë©´ ë‚˜ì¤‘ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡ ì €ì¥
        test_dataset = test_dataset_raw  # transformì€ Noneì´ì§€ë§Œ ë‚˜ì¤‘ì— ì ìš©
else:
    # ì „ì²´ ë°ì´í„° ì‚¬ìš©
    if 'val_transform' in globals() and val_transform is not None:
        test_dataset = CIFAR10Dataset(
            data_dir=test_data.data_dir,
            train=False,
            transform=val_transform
        )
    else:
        test_dataset = CIFAR10Dataset(
            data_dir=test_data.data_dir,
            train=False,
            transform=None
        )

if USE_SUBSET:
    print(f'âœ… Total train size: {len(train_dataset_raw):,}')
    print(f'âœ… Test size: {len(test_dataset_raw):,}')
else:
    print(f'âœ… Total train size: {len(train_data):,}')
    print(f'âœ… Test size: {len(test_dataset):,}')

if USE_SUBSET:
    print(f"\nğŸ’¡ ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµí•˜ë ¤ë©´ USE_SUBSET = Falseë¡œ ë³€ê²½í•˜ì„¸ìš”")