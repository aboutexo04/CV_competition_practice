# src/data.py

import torch
from torch.utils.data import DataLoader, Dataset
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np
from pathlib import Path
import pandas as pd
from PIL import Image

# ============================================
# Augmentation í•¨ìˆ˜ë“¤
# ============================================

def get_albumentations_train(img_size=224):
    """ì¼ë°˜ ì´ë¯¸ì§€ìš© augmentation"""
    return A.Compose([
        A.Resize(img_size, img_size),
        
        # Geometric
        A.Rotate(limit=5, p=0.3),
        A.ShiftScaleRotate(
            shift_limit=0.05,
            scale_limit=0.05,
            rotate_limit=5,
            p=0.3
        ),
        A.Perspective(scale=0.05, p=0.3),
        
        # Color & Brightness
        A.RandomBrightnessContrast(
            brightness_limit=0.2,
            contrast_limit=0.2,
            p=0.5
        ),
        
        # Noise & Blur
        A.GaussNoise(p=0.2),
        A.Blur(blur_limit=3, p=0.2),
        
        # Normalize
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
        ToTensorV2()
    ])


try:
    from augraphy import (
        InkBleed, PaperFactory, DirtyDrum,
        Jpeg, Brightness, AugraphyPipeline
    )
    AUGRAPHY_AVAILABLE = True
except ImportError:
    AUGRAPHY_AVAILABLE = False


def get_augraphy_train(img_size=224):
    """ë¬¸ì„œ íŠ¹í™” augmentation (Augraphy)"""
    if not AUGRAPHY_AVAILABLE:
        print("âš ï¸  Augraphy not installed. Falling back to Albumentations.")
        return get_albumentations_train(img_size)
    
    # Augraphy íŒŒì´í”„ë¼ì¸
    ink_phase = [
        InkBleed(intensity_range=(0.1, 0.3), p=0.3),
    ]
    
    paper_phase = [
        PaperFactory(p=0.3),
        DirtyDrum(p=0.2),
    ]
    
    post_phase = [
        Jpeg(quality_range=(60, 95), p=0.3),
        Brightness(brightness_range=(0.95, 1.05), p=0.3),
    ]
    
    augraphy_pipeline = AugraphyPipeline(ink_phase, paper_phase, post_phase)
    
    return A.Compose([
        A.Lambda(image=lambda x, **kwargs: augraphy_pipeline.augment(x)["output"]),
        A.Resize(img_size, img_size),
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
        ToTensorV2()
    ])


def get_hybrid_train(img_size=224, augraphy_strength='light'):
    """Augraphy + Albumentations í˜¼í•©"""
    
    if not AUGRAPHY_AVAILABLE:
        print("âš ï¸  Augraphy not installed. Falling back to Albumentations.")
        return get_albumentations_train(img_size)

    # ê°•ë„ë³„ í™•ë¥ 
    if augraphy_strength == 'light':
        ink_p, paper_p, post_p = 0.2, 0.2, 0.2
    elif augraphy_strength == 'medium':
        ink_p, paper_p, post_p = 0.4, 0.4, 0.3
    else:  # heavy
        ink_p, paper_p, post_p = 0.6, 0.5, 0.4
    
    # Augraphy íŒŒì´í”„ë¼ì¸
    ink_phase = [InkBleed(intensity_range=(0.05, 0.15), p=ink_p)]
    paper_phase = [PaperFactory(p=paper_p), DirtyDrum(p=paper_p * 0.5)]
    post_phase = [
        Jpeg(quality_range=(70, 95), p=post_p),
        Brightness(brightness_range=(0.95, 1.05), p=post_p)
    ]
    
    augraphy_pipeline = AugraphyPipeline(ink_phase, paper_phase, post_phase)
    
    return A.Compose([
        # Augraphy ì ìš©
        A.Lambda(image=lambda x, **kwargs: augraphy_pipeline.augment(x)["output"]),
        
        # ì¼ë°˜ augmentation
        A.Rotate(limit=3, p=0.4),
        A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.4),
        A.GaussNoise(var_limit=(5, 30), p=0.2),
        
        # ì „ì²˜ë¦¬
        A.Resize(img_size, img_size),
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
        ToTensorV2()
    ])


def get_val_augmentation(img_size=224):
    """ê²€ì¦ìš© augmentation (ë³€í™˜ ì—†ìŒ)"""
    return A.Compose([
        A.Resize(img_size, img_size),
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ),
        ToTensorV2()
    ])


def get_train_augmentation(config):
    """
    Config ê¸°ë°˜ìœ¼ë¡œ augmentation ìë™ ì„ íƒ
    
    Args:
        config: Config ê°ì²´
    """
    strategy = config.AUG_STRATEGY
    dataset_type = config.DATASET_TYPE
    img_size = config.IMAGE_SIZE
    
    # Auto ëª¨ë“œ: ë°ì´í„°ì…‹ì— ë§ê²Œ ìë™ ì„ íƒ
    if strategy == 'auto':
        if dataset_type in ['document', 'text', 'ocr']:
            strategy = 'hybrid'
        else:
            strategy = 'albumentations'
        
        print(f"ğŸ“Œ Auto mode: {dataset_type} â†’ {strategy} augmentation")
    
    # ì „ëµë³„ augmentation
    if strategy == 'albumentations':
        return get_albumentations_train(img_size)
    elif strategy == 'augraphy':
        return get_augraphy_train(img_size)
    elif strategy == 'hybrid':
        augraphy_strength = getattr(config, 'AUGRAPHY_STRENGTH', 'light')
        return get_hybrid_train(img_size, augraphy_strength=augraphy_strength)
    else:
        raise ValueError(f"Unknown strategy: {strategy}")


# ============================================
# Document Dataset í´ë˜ìŠ¤
# ============================================

class DocumentDataset(Dataset):
    """
    ë¬¸ì„œ ë¶„ë¥˜ ëŒ€íšŒìš© Dataset
    
    Args:
        df: train.csv DataFrame (ID, target ì»¬ëŸ¼)
        img_dir: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        transform: Albumentations transform
        indices: ì‚¬ìš©í•  ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ (K-Foldìš©, Noneì´ë©´ ì „ì²´)
    """
    def __init__(self, df, img_dir, transform=None, indices=None):
        self.img_dir = Path(img_dir)
        self.transform = transform
        
        if indices is not None:
            self.df = df.iloc[indices].reset_index(drop=True)
            self.indices = indices
        else:
            self.df = df
            self.indices = list(range(len(df)))
        
        self.image_ids = self.df['ID'].tolist()
        self.labels = self.df['target'].tolist()
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ë¡œë“œ
        img_id = self.image_ids[idx]
        img_path = self.img_dir / img_id
        
        try:
            image = Image.open(img_path).convert('RGB')
            image = np.array(image)
        except Exception as e:
            print(f"âš ï¸  Error loading image {img_path}: {e}")
            image = np.zeros((224, 224, 3), dtype=np.uint8)
        
        label = int(self.labels[idx])
        
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        
        return image, label


# ============================================
# DataLoader ìƒì„±
# ============================================

def get_dataloaders(train_dataset_raw, train_labels, test_dataset, config):
    """
    Config ê¸°ë°˜ìœ¼ë¡œ DataLoader ìƒì„±
    
    Args:
        train_dataset_raw: Transform ì—†ëŠ” train dataset
        train_labels: Train labels (K-Foldìš©)
        test_dataset: Transform ì ìš©ëœ test dataset
        config: Config ê°ì²´
        
    Returns:
        test_loader (K-Foldì—ì„œ ì§ì ‘ train_loader ìƒì„±)
    """
    # test_datasetì´ Noneì´ë©´ (ëŒ€íšŒ ì´ˆê¸°) None ë°˜í™˜
    if test_dataset is None:
        return None
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=0  # MPS í˜¸í™˜ì„±
    )
    
    return test_loader


# ============================================
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# ============================================

def load_document_data(config):
    """
    ë¬¸ì„œ ë¶„ë¥˜ ëŒ€íšŒ ë°ì´í„° ë¡œë“œ
    
    ë°ì´í„° êµ¬ì¡°:
    - train.csv: ID, target (ì´ë¯¸ì§€ëª… <-> í´ë˜ìŠ¤ ì¸ë±ìŠ¤)
    - meta.csv: target, class_name (í´ë˜ìŠ¤ ì¸ë±ìŠ¤ <-> í´ë˜ìŠ¤ëª…)
    - train/: í•™ìŠµ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
    
    Args:
        config: Config ê°ì²´
        
    Returns:
        train_dataset_raw, test_dataset, train_labels, class_names, num_classes
    """
    print("="*60)
    print("ğŸ“„ Loading Document Classification Data")
    print("="*60)
    
    # ë°ì´í„° ê²½ë¡œ ì„¤ì •
    current_file = Path(__file__).resolve()
    project_root = current_file.parent.parent
    data_dir = project_root / 'data' / 'document'
    
    train_csv_path = data_dir / 'train.csv'
    meta_csv_path = data_dir / 'meta.csv'
    train_img_dir = data_dir / 'train'
    
    # CSV íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not train_csv_path.exists():
        raise FileNotFoundError(
            f"âŒ train.csv not found at {train_csv_path}\n"
            f"Please download competition data and place it in {data_dir}"
        )
    
    if not meta_csv_path.exists():
        raise FileNotFoundError(
            f"âŒ meta.csv not found at {meta_csv_path}\n"
            f"Please download competition data and place it in {data_dir}"
        )
    
    # CSV ë¡œë“œ
    train_df = pd.read_csv(train_csv_path)
    meta_df = pd.read_csv(meta_csv_path)
    
    # í´ë˜ìŠ¤ ì •ë³´ ì¶”ì¶œ
    meta_df = meta_df.sort_values('target').reset_index(drop=True)
    class_names = meta_df['class_name'].tolist()
    num_classes = len(class_names)
    
    print(f"\nâœ… Document Data Loaded!")
    print(f"Total samples: {len(train_df):,}")
    print(f"Number of classes: {num_classes}")
    print(f"Class names: {class_names[:5]}{'...' if num_classes > 5 else ''}")
    
    # Dataset ìƒì„±
    train_dataset_raw = DocumentDataset(
        df=train_df,
        img_dir=train_img_dir,
        transform=None
    )
    train_labels = train_df['target'].tolist()
    
    # Test ë°ì´í„°ëŠ” ë‚˜ì¤‘ì— ì œê³µë˜ë©´ ì¶”ê°€
    test_dataset = None
    
    return train_dataset_raw, test_dataset, train_labels, class_names, num_classes


def load_data(config):
    """
    Config ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ìë™ ë¡œë“œ
    
    Args:
        config: Config ê°ì²´
        
    Returns:
        train_dataset_raw, test_dataset, train_labels, class_names, num_classes
    """
    print(f"\nğŸ¯ Dataset Type: {config.DATASET_TYPE}")
    
    if config.DATASET_TYPE == 'document':
        return load_document_data(config)
    else:
        raise ValueError(
            f"Unknown dataset type: {config.DATASET_TYPE}\n"
            f"Available types: 'document'"
        )